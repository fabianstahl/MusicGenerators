\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{caption}

\begin{document}

\setlength\parindent{0pt}
\title{Raw Audio Music Generation Using Deep Neural Networks}
\author{Fabian Stahl}


\maketitle

\tableofcontents

\newpage
\section*{Abbreviations}
\begin{center}
\begin{tabular}{l l}
\textbf{CNN} & Convolutional neural network \\
\textbf{DCGAN} & Deep Convolutional Generative Adverserial Networks\\
\textbf{GAN} & Generative adversarial network \\
\textbf{GRU} & Gated Recurrent Unit \\
\textbf{KLD}  & Kullback–Leibler divergence\\
\textbf{LSTM} & Long short-term memory \\
\textbf{MIDI} & Musical Instrument Digital Interface \\
\textbf{MLP} & Multilayer Perceptrons \\
\textbf{ReLU} & Rectified Linear Unit \\
\textbf{RNN} & Recurrent neural network \\
\textbf{VAE} & Variational autoencoder \\
\textbf{VRNN} & Variational Recurrent neural network \\
\end{tabular}
\end{center}
\newpage


\begin{abstract}
Music can be found in almost every movie, video game or public location.
Its targeted use can change people's mood, encourage buying decisions or add context to accompanying content.
However, composing, recording and mixing music is a creative process, that takes a lot of time and skill to master.
This poses the question, if pleasing music can be generated autonomously.

Deep neural networks have been known to solve a wide range problems.
During this semester's project I will compare different network architectures to generate sample based music, show how they can be trained and rate results.
\end{abstract}

\section{Related Work}
% midi
In the field of audio generation there is a wide variety of approaches.
Most of them work with symbolic music representations, such as Ascii-text, musical scores or spectrogram data.
Especially the MIDI format is commonly used, a binary format specifying musical notes.
This makes the format very slim and easy to work with.
MIDI-based approaches include e.g. GANs with convolutions layers ~\cite{yang2017midinet, mogren2016c, mogren2016c}, VAEs paired with LSTMs~\cite{roberts2018hierarchical, tikhonov2017music, hennig2017classifying} and RNNs with restricted Boltzmann machines \cite{boulanger2012modeling}.
However, musical nuances like tonal colors, scratch and breath noises or intonations are necessary to make most music genres more interesting.
Having MIDI output only, the musical interpretation, (the raw audio sent to the sound port), is up to an external MIDI synthesizer.

There are commercial music streaming services that claim to provide generated music that fits the customer's mood.
The most prominent of them being brain.fm~\cite{brain.fm} and \cite{Mubert}.
However, the brain.fm algorithm only learns certain tasks, such as arrangin motifs over long timescales, while melodies, instrument choices and chord progressions are precomposed by humans.
Mubert also uses handcrafted single instrument MIDI loops, but generates some MIDI layers using decision trees, random forests and musical analysis.
Their music is limited to electronic genres only, which narrows musical variety.
While these application developers like to emphasise the machine learning aspect, their algorithms are not fully autonomous.
The two biggest reasons for that could be the high complexity of generating music sample by sample, especially with real time constraints, as well as the ineptitude of the synthesized music for commercial use.




\section{SampleRNN}
Recurrent Neural Networks (RNNs) are used to process sequences of data.
Output data is not only based upon input data, but also on a hidden state vector, that encodes previous input.
RNNs consist of arrays of small memory units.
While the original \emph{vanilla cells} were prone to the vanishing gradient problem, more advanced structures like LSTMs and GRUs could learn to keep important information and forget redundant or irrelevant previous input.
Common RNNs don't scale very good with the high temporal resolution, which is required for sound output.
Corralation can exist between neighbouring samples as well as between samples that are more than thousand frames apart.

Mehri et al. adresses this issue by using multiple stacked RNNs with different temporal resolutions (see \ref{fig::samplernn} for an example model).
Their so called \textbf{SampleRNN}~\cite{mehri2016samplernn} is a promising candidate for music generation.
Stacked \emph{Frame-Level Modules} with different fields of perception are used to capture temporal context over different time periods.
State vectors are passed to the next lower module.
A \emph{Sample-Level Module} puts their state vector into consideration as well as the current input sample to generate a probability distribution for the next sample.
Originally tested only for piano music, \cite{zukowski2018generating} and \cite{carr2018generating} showed, that SampleRNN is especially good to generate loud music genres, like Metal and Dark Ambient.
Its biggest drawback is, that sample quality varies strongly even after days of training.




\subsection{Architecture}
\begin{figure}[ht]
    \label{fig:samplernn}
    \includegraphics[width=\textwidth]{img/samplernn.png}
    \caption{The SampleRNN consist of multiple Frame-Level Modules, two in this example, and a Sample-Level Module (tier 1). Here, tier 3 has a field of perception of 15 samples, whereas tier 2 uses 3 samples each step and needs to be called more frequently. The state vector $c$ of tier 3 is passed to tier 2 and combined with its state vector $h$. Tier 1 takes the product of $c$ and the sample input and feeds it trough a MLP to determine a probability distribution to draw the next sample. Note that this example is simplified. }
\end{figure}









\section{VRNN}

% Idee (Markov, variational autoencoder erklären)
\subsection{Idea}
Another approach to generate sound data is the Variational Recurrent Neural Network (VRNN), proposed 2016 by Chung et al.~\cite{chung2015recurrent}.
The architecture was trained on the tasks of handwriting and sound generation.
However, experiments for the ladder only included training with speech datasets and non-linguistic human-made sounds like coughing and screaming.
This poses the question if the VRNN can be used to generate music as well.

Chung et al. states that there is an issue in the way that RNNs model output variety, since the only source of true randomness is drawing values from an output probability density.
Data that is both highly variable and highly structered can be hard to model this way.
On the one hand small input variations should lead to potentially very large variations in the hidden state vector $h_t$, on the other hand, different high level structures need to be mapped to distinct state vectors as well.
This forces the network to compromise between the encoding of low-level and high-level features.
The main idea of the VRNN is to extend a recurrent neural network with latent random variables, therefore improving the representational power of the model.
Such high-level variables are commonly used in VAEs to model multimodal conditional distributions, approximating real world data variety~\cite{kingma2013auto}.
% <Satz zu VAEs>
Chung et al. proposes to extend the VEA model by inducing temporal dependencies using the hidden state vector of a RNN across neighboring timesteps.



 
% Architektur erklären (Diagramme, Fehlerfunktionen, Trainingsdiagramme)



\subsection{Training}

\begin{figure}
\includegraphics{vrnn-arch.png}
\label{fig:vrnn-arch}
\caption{
This image shows all necessary steps for a training cycle.
First the raw audio data is passed through $\phi^x$ to extract features from $x_t$ (1).
Since the KLD is used to make the $\phi^{prior}$ and $\phi^{enc}$ as similar as possible, the prior needs to be included in the model.
It handles the state vector $h_t$ (2).
The encoder $\phi^{enc}$ is used during training only.
It takes the concatenated $\phi^x$ and the state vector $h_t$ (3).
Next two single layer networks are used to calculate the standard deviation and the mean value of the generated distribution and a random sample $z$ is drawn from it (4).
Another network $\phi^z$ is used to find features in $z$ (5).
The decoder tries to replicate the mean and standard deviation of the input data. % ???
It takes the concatenated $\phi^z$ and $h_t$ (6).
$z$ and $h$ are passed to the RNN to determine a new state vector $h_{t+1}$ (7).
Lastly the overall loss is calculated (8).
It is composed of the KLD from $\phi^{prior}$ and $\phi^{dec}$ to make them similar, as well as a function to compare $x$ to $x:out$ % ???
}
\end{figure}





% Describe p, q, ...

conditional prior / generative model - p(x | z)
approximate posterior / inference model - q(z | x)
prior over latent random variables - p(z)


 
The model can be trained by minimizing the Kullback–Leibler divergence (KLD) - first term - and maximizing the likelihood of an $x$ given a $z$ - second term).
This can be seen as adding a regulizer to a simple auto encoder to ensure that the generated distributions have a certain form.
$$
E_{\mathbf{z} \le T | \mathbf{x} \le T} = \left[ \sum_{t=1}^T -(KL(q(\mathbf{z}_t | \mathbf{x}_{\le t}, \mathbf{z}_{<t}) || p(\mathbf{z}_t | \mathbf{x}_{<t}, \mathbf{z}_{<t})) + log(p(\mathbf{x}_t | \mathbf{z}_{\le t, \mathbf{x}_{<t}}))) \right]
$$

The KLD between the two distributions $Q$ and $P$ with respect to $Q$ can be seen as a measurment of similarity between two distributions \cite{}.
Since the probabilistic Encoder $q(z|x)$ is used to approximate the hard to compute posterior $p(x|z)$, $Q$ needs to be as similar to $P$ as possible.
This means that the KL divergence needs to be minimized, which is identical to maximizing the lower variational bound~\cite{kingma2013auto}. 
%The KLD is defined as:
%$$
%KL(q || p) = -\sum p(x) log(\frac{p(x)}{q(x)})  
%$$

The second term of the error function describes the reconstruction error.
%$$
%log p(\mathbf{x}_t | \mathbf{z}_{\le t, \mathbf{x}_{<t}})
%$$

In theory the model can be trained using any probability distribution.
The original paper uses gaussian distributions, leading to
$$
KL(q(\mathbf{z}_t | \mathbf{x}_{\le t}, \mathbf{z}_{<t}) || p(\mathbf{z}_t | \mathbf{x}_{<t}, \mathbf{z}_{<t}))
= \sum \frac{log(\sigma_2)}{log(\sigma_1)}+\frac{\sigma_1^2 + (\mu_1 - \mu_2)}{2 \sigma_2^2} - \frac{1}{2}
$$
for term 1, where $\mu_1$ and $\mu_2$ are mean values and $\sigma_1$ and $\sigma_2$ are are the standard deviations of the two distributions $p$ and $q$ (see \cite{klproof} for a proof ), and
$$
log(p(\mathbf{x}_t | \mathbf{z}_{\le t, \mathbf{x}_{<t}}))
= \sum \frac{\sqrt{y-\mu}}{2 \sigma^2} + log(\sigma) + \frac{log(2\pi)}{2}
$$
describes the second term, where %To_do!


% Implementierungsdetaills
Since the original source code was using the Theano ML library, I cloned a public PyTorch implementation~\cite{VRNNgit}.
However, I found out, that I needed to make quite many adaptions to fit the original version.
The biggest changes include a new data loader class, adding the correct error function, using more dense layers in all components and make the code more GPU-friendly. 


% Results

\subsubsection{Architecture}
\subsubsection{Training}









\section{WaveGAN}
In 2014 Goodfellow et al. introduced a new approach to train networks called Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}.
Such architectures consist of a generator and a discriminator sub-network.
While the discriminator tries to distinguish between real and generated data, it is the generator?s task to fool the discriminator into thinking its output is real.
Both networks are trained simultaneously and try to outperform each other.

While GANs have been used widely for tasks related to images, there are only few approaches using audio data.
Donahue et al. presented two GAN approaches to generate sound data called \emph{WaveGAN} and \emph{SpecGAN}~\cite{donahue2018adversarial}.
Both models are adaptions of a model class called Deep Convolutional Generative Adverserial Networks or DCGANs, first proposed by Radford et al.~\cite{radford2015unsupervised}.
The SpecGAN works with spectrogram images that are processed by the network in a similar way to normal images.
However, the final conversion from image to sound data is known to be problematic, since a lot of noise is induced.
The second architecture, the WaveGAN, is a flattened adaption of the original DCGAN model using 1D convolutions on raw audio samples.
Both models have the same number of parameters and numerical operations.
However, neither strategy has been tested with music datasets.
Since SpecGANs lossy conversion poses difficulties beyond the actual AI model, only the second approach will be presented here.

Unfortunately the original model description is only capable to generate about a second of audio.
By adding one more transposed convolution layer to the encoder, one convolution layer to the decoder, and increasing the number of intermediate output channels, the models generating capacity can be increased to about 4 seconds, possibly
enabling whole musical motifs. 
This expanded model can be found on the authors GIT repository\cite{donahue2019wavegan}.



\subsection{Architecture}
The generator network is trained to map a low-dimensional latent vector $z$ to a high-dimensional data output.
First the network applies a fully connected layer with $512d$ output nodes to $z$, where $d$ is used to adjust the models size.
The output vector is then reshaped, since an additional dimension is needed for the sound channel.
The signal then passes through 5 transposed 1D convolution layers with stride 4 and ReLU activation functions in between as well as a Tanh function at the end.

The decoder is basically a mirrored generator with a few exceptions.
Instead of ReLUs, Leaky ReLUs with $\alpha=0.2$ are used in between convolution layers.
Also a new kind of layer, called PhaseShuffle, is introduced here. 
Since the generators transposed convolution is known to produce artifacts, the discriminator may learn to find generated samples based on these artifacts instead of conducting a broad content analysis.
Shifting the vector randomly up to $n$ times in a circle can prevent this learning behaviour.
See figure \ref{fig:WaveGAN-arch} for a visualization of the graph with parameters.


% To-Do!
\begin{figure}
\includegraphics[width=0.9\textwidth]{img/wavegan-arch.png}
\label{fig:WaveGan-arch}
\caption{Architecture of the WaveGAN, where $d$ adjusts the models size, $n$ is the batch size and $c$ is the number of output channels.}%To-Do
\end{figure}

\subsection{Training}

Training GANs can be seen as a two player minimax game. 
While the generator $G$ is trained to minimize the following value function, the discriminator
$D_w$ is trained to maximize it.

$$
V_{WGAN}(D_w, G) = E_{x \sim P_x}[D_w(x)] - E_{z \sim P_z}[D_w(G(z))]
$$

The network is trained using the Wasserstein-GAN Gradient Penalty, or short \emph{WGAN-GP}, strategy.
That means, that instead of predicting probabilities, the discriminator is trained to assist computing the wasserstein or
earth movers distance between the distribution of generated data and distribution of real examples.
However, minimizing this measure is only possible if the discriminator function is 1-Lipschitz, which can be done by enforcing a gradient penalty.
For more details on the WGAN-GP stategy see~\cite{gulrajani2017improved}.

In most experiments training was stopped in between $40.000$ and $50.000$ steps since no more progress was achieved.
On a NVIDIA RTX2080 GPU, this took about a day and 7 of 8Gb RAM.
All other parameter choices are similar to [?].
Adam Optimizer with $learning rate = 1e ? 4$, $?1 = 0.5$ and $?2 = 0.9$
was used for both discriminator and generator.
The model was trained using both a batch size $b$ and a scale factor $d$ of 64.


\section{Evaluation}
SampleRNN, VRNN and WaveGAN have been rained on
\begin{itemize}
 \item piano music (Felix Mendelssohn: Lieder ohne Worte, about 2 hours)
 \item instrumental metal (Intervals: The way forward + The shape of colour, about ??? hours)
 \item hip-hop (Cypress Hill: IV, 1 hour and 13 minutes)
\end{itemize}

For SampleRNN and VRNN the music was split into 8 second and for WaveGAN into 4 second chunks.
Audio examples can be found in the repository folder examples. % TO-DO!



\section{Discussion}
Despite GANS being often hard to train, WaveGAN has the most simple architecture out of all three models.
Unlike both other autoregressive models, where samples have to be fed back into the model, the WaveGAN audio generation is feed-forward only, which makes generation extremely fast.
It also is the only architecture that is (out-of-the-box) capable of producing multi-channel sound output.
However, the main drawback of the WaveGAN is the models limitation to generate music of more than a couple seconds.
Regarding memory and complexity requirements compared to the other models, WaveGAN is very hard to scale.

In all experiments the model did learn to produce music that somewhat showed features from the input data.
In Samples one can often observe instruments, small riffs and drum hits.
However, in all experiments the networks never fully learned to reduce noise.
This can clearly be heard in the examples and often even dominates them.
Increasing the model and batch size on a GPU with more RAM might help.




\end{document}
